# Database backup and disaster recovery procedures
apiVersion: v1
kind: ConfigMap
metadata:
  name: backup-scripts
  namespace: fullstack-monolith
data:
  backup-database.sh: |
    #!/bin/bash

    # PostgreSQL backup script with rotation and compression

    set -euo pipefail

    # Configuration
    DB_HOST="${DB_HOST:-postgres-service}"
    DB_PORT="${DB_PORT:-5432}"
    DB_NAME="${DB_NAME:-fullstack_production}"
    DB_USER="${DB_USER:-postgres}"
    BACKUP_DIR="${BACKUP_DIR:-/backups}"
    S3_BUCKET="${S3_BUCKET:-fullstack-monolith-backups}"
    RETENTION_DAYS="${RETENTION_DAYS:-30}"

    # Create backup directory
    mkdir -p "$BACKUP_DIR"

    # Generate backup filename with timestamp
    TIMESTAMP=$(date +%Y%m%d_%H%M%S)
    BACKUP_FILE="$BACKUP_DIR/postgres_backup_${TIMESTAMP}.sql"
    COMPRESSED_FILE="${BACKUP_FILE}.gz"

    echo "Starting PostgreSQL backup at $(date)"
    echo "Database: $DB_NAME"
    echo "Host: $DB_HOST:$DB_PORT"

    # Create database dump
    echo "Creating database dump..."
    pg_dump -h "$DB_HOST" -p "$DB_PORT" -U "$DB_USER" -d "$DB_NAME" \
        --verbose --no-password --format=custom --compress=9 \
        --file="$BACKUP_FILE"

    # Compress the backup
    echo "Compressing backup..."
    gzip "$BACKUP_FILE"

    # Calculate backup size
    BACKUP_SIZE=$(du -h "$COMPRESSED_FILE" | cut -f1)
    echo "Backup size: $BACKUP_SIZE"

    # Upload to S3 if configured
    if [ -n "${AWS_ACCESS_KEY_ID:-}" ] && [ -n "${S3_BUCKET:-}" ]; then
        echo "Uploading to S3..."
        aws s3 cp "$COMPRESSED_FILE" "s3://$S3_BUCKET/postgres/$(basename $COMPRESSED_FILE)"
        
        # Set lifecycle policy for automatic cleanup
        aws s3api put-object-tagging \
            --bucket "$S3_BUCKET" \
            --key "postgres/$(basename $COMPRESSED_FILE)" \
            --tagging "TagSet=[{Key=backup-type,Value=database},{Key=retention-days,Value=$RETENTION_DAYS}]"
    fi

    # Clean up old local backups
    echo "Cleaning up old backups..."
    find "$BACKUP_DIR" -name "postgres_backup_*.sql.gz" -mtime +$RETENTION_DAYS -delete

    # Verify backup integrity
    echo "Verifying backup integrity..."
    if gzip -t "$COMPRESSED_FILE"; then
        echo "✓ Backup integrity verified"
    else
        echo "✗ Backup integrity check failed"
        exit 1
    fi

    echo "Backup completed successfully at $(date)"
    echo "Backup file: $COMPRESSED_FILE"

  restore-database.sh: |
    #!/bin/bash

    # PostgreSQL restore script

    set -euo pipefail

    # Configuration
    DB_HOST="${DB_HOST:-postgres-service}"
    DB_PORT="${DB_PORT:-5432}"
    DB_NAME="${DB_NAME:-fullstack_production}"
    DB_USER="${DB_USER:-postgres}"
    BACKUP_FILE="${1:-}"

    if [ -z "$BACKUP_FILE" ]; then
        echo "Usage: $0 <backup_file>"
        echo "Available backups:"
        ls -la /backups/postgres_backup_*.sql.gz 2>/dev/null || echo "No backups found"
        exit 1
    fi

    if [ ! -f "$BACKUP_FILE" ]; then
        echo "Error: Backup file '$BACKUP_FILE' not found"
        exit 1
    fi

    echo "Starting PostgreSQL restore at $(date)"
    echo "Backup file: $BACKUP_FILE"
    echo "Target database: $DB_NAME"

    # Create a backup of current database before restore
    CURRENT_BACKUP="/tmp/pre_restore_backup_$(date +%Y%m%d_%H%M%S).sql"
    echo "Creating backup of current database..."
    pg_dump -h "$DB_HOST" -p "$DB_PORT" -U "$DB_USER" -d "$DB_NAME" \
        --format=custom --file="$CURRENT_BACKUP"

    # Drop existing connections
    echo "Terminating existing connections..."
    psql -h "$DB_HOST" -p "$DB_PORT" -U "$DB_USER" -d postgres -c \
        "SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE datname = '$DB_NAME' AND pid <> pg_backend_pid();"

    # Drop and recreate database
    echo "Dropping and recreating database..."
    psql -h "$DB_HOST" -p "$DB_PORT" -U "$DB_USER" -d postgres -c "DROP DATABASE IF EXISTS $DB_NAME;"
    psql -h "$DB_HOST" -p "$DB_PORT" -U "$DB_USER" -d postgres -c "CREATE DATABASE $DB_NAME;"

    # Restore from backup
    echo "Restoring from backup..."
    if [[ "$BACKUP_FILE" == *.gz ]]; then
        gunzip -c "$BACKUP_FILE" | pg_restore -h "$DB_HOST" -p "$DB_PORT" -U "$DB_USER" -d "$DB_NAME" --verbose
    else
        pg_restore -h "$DB_HOST" -p "$DB_PORT" -U "$DB_USER" -d "$DB_NAME" --verbose "$BACKUP_FILE"
    fi

    # Verify restore
    echo "Verifying restore..."
    TABLE_COUNT=$(psql -h "$DB_HOST" -p "$DB_PORT" -U "$DB_USER" -d "$DB_NAME" -t -c \
        "SELECT count(*) FROM information_schema.tables WHERE table_schema = 'public';")

    echo "Restored $TABLE_COUNT tables"
    echo "Restore completed successfully at $(date)"
    echo "Pre-restore backup saved to: $CURRENT_BACKUP"

  disaster-recovery.sh: |
    #!/bin/bash

    # Disaster recovery script

    set -euo pipefail

    echo "=== DISASTER RECOVERY PROCEDURE ==="
    echo "Started at: $(date)"

    # Step 1: Assess the situation
    echo "Step 1: Assessing system status..."

    # Check if database is accessible
    if pg_isready -h "$DB_HOST" -p "$DB_PORT"; then
        echo "✓ Database is accessible"
        DB_ACCESSIBLE=true
    else
        echo "✗ Database is not accessible"
        DB_ACCESSIBLE=false
    fi

    # Check if Redis is accessible
    if redis-cli -h "$REDIS_HOST" -p "$REDIS_PORT" ping | grep -q PONG; then
        echo "✓ Redis is accessible"
        REDIS_ACCESSIBLE=true
    else
        echo "✗ Redis is not accessible"
        REDIS_ACCESSIBLE=false
    fi

    # Step 2: Determine recovery strategy
    echo "Step 2: Determining recovery strategy..."

    if [ "$DB_ACCESSIBLE" = false ]; then
        echo "Database recovery required"
        
        # Find latest backup
        LATEST_BACKUP=$(ls -t /backups/postgres_backup_*.sql.gz 2>/dev/null | head -1)
        if [ -n "$LATEST_BACKUP" ]; then
            echo "Latest backup found: $LATEST_BACKUP"
            echo "Initiating database restore..."
            ./restore-database.sh "$LATEST_BACKUP"
        else
            echo "No local backups found. Checking S3..."
            if [ -n "${S3_BUCKET:-}" ]; then
                aws s3 sync "s3://$S3_BUCKET/postgres/" /backups/
                LATEST_BACKUP=$(ls -t /backups/postgres_backup_*.sql.gz 2>/dev/null | head -1)
                if [ -n "$LATEST_BACKUP" ]; then
                    echo "Downloaded backup from S3: $LATEST_BACKUP"
                    ./restore-database.sh "$LATEST_BACKUP"
                else
                    echo "No backups available. Manual intervention required."
                    exit 1
                fi
            fi
        fi
    fi

    if [ "$REDIS_ACCESSIBLE" = false ]; then
        echo "Redis recovery required"
        # Redis recovery is typically handled by Kubernetes restart
        echo "Restarting Redis service..."
        kubectl rollout restart deployment/redis -n fullstack-monolith
    fi

    # Step 3: Verify recovery
    echo "Step 3: Verifying recovery..."

    # Wait for services to be ready
    sleep 30

    # Run health checks
    ./health-check.sh

    echo "Disaster recovery completed at: $(date)"

---
apiVersion: batch/v1
kind: CronJob
metadata:
  name: database-backup
  namespace: fullstack-monolith
spec:
  schedule: '0 2 * * *' # Daily at 2 AM
  jobTemplate:
    spec:
      template:
        spec:
          containers:
            - name: backup
              image: postgres:15-alpine
              command:
                - /bin/sh
                - -c
                - |
                  apk add --no-cache aws-cli gzip
                  /scripts/backup-database.sh
              env:
                - name: DB_HOST
                  value: 'postgres-service'
                - name: DB_NAME
                  value: 'fullstack_production'
                - name: DB_USER
                  valueFrom:
                    secretKeyRef:
                      name: app-secrets
                      key: POSTGRES_USER
                - name: PGPASSWORD
                  valueFrom:
                    secretKeyRef:
                      name: app-secrets
                      key: POSTGRES_PASSWORD
                - name: AWS_ACCESS_KEY_ID
                  valueFrom:
                    secretKeyRef:
                      name: aws-credentials
                      key: access-key-id
                - name: AWS_SECRET_ACCESS_KEY
                  valueFrom:
                    secretKeyRef:
                      name: aws-credentials
                      key: secret-access-key
                - name: S3_BUCKET
                  value: 'fullstack-monolith-backups'
              volumeMounts:
                - name: backup-scripts
                  mountPath: /scripts
                - name: backup-storage
                  mountPath: /backups
          volumes:
            - name: backup-scripts
              configMap:
                name: backup-scripts
                defaultMode: 0755
            - name: backup-storage
              persistentVolumeClaim:
                claimName: backup-pvc
          restartPolicy: OnFailure

---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: backup-pvc
  namespace: fullstack-monolith
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 50Gi
